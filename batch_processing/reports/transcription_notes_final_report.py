#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è½‰éŒ„ç­†è¨˜è™•ç†æœ€çµ‚å ±å‘Š
"""

import os
import json
from pathlib import Path
from datetime import datetime


def main():
    base_path = "/Volumes/WD_BLACK/åœ‹éš›å¹´æœƒ/ADA2025"
    
    print("\nğŸ“Š ADA2025 è½‰éŒ„ç­†è¨˜è™•ç†æœ€çµ‚å ±å‘Š")
    print("="*80)
    print(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)
    
    # è¼‰å…¥é€²åº¦
    progress_file = 'transcription_notes_progress_v2.json'
    if os.path.exists(progress_file):
        with open(progress_file, 'r') as f:
            progress = json.load(f)
    else:
        print("æ‰¾ä¸åˆ°é€²åº¦æ–‡ä»¶")
        return
    
    # çµ±è¨ˆ
    print("\nğŸ“ˆ è™•ç†çµ±è¨ˆï¼š")
    print(f"  âœ… æˆåŠŸè™•ç†: {len(progress['processed'])} å€‹æ–‡ä»¶")
    print(f"  âŒ è™•ç†å¤±æ•—: {len(progress['failed'])} å€‹æ–‡ä»¶")
    print(f"  ğŸ’° ç¸½ Token ä½¿ç”¨: {progress['stats'].get('total_tokens', 0):,}")
    
    # è¨ˆç®—æˆæœ¬ï¼ˆä¼°ç®—ï¼‰
    # Gemini 2.5 Pro åƒ¹æ ¼ç´„ç‚º $7.5/ç™¾è¬ tokens
    total_tokens = progress['stats'].get('total_tokens', 0)
    estimated_cost = (total_tokens / 1_000_000) * 7.5
    print(f"  ğŸ’µ ä¼°è¨ˆæˆæœ¬: ${estimated_cost:.2f} USD")
    
    # æŸ¥æ‰¾ç”Ÿæˆçš„ç­†è¨˜æ–‡ä»¶
    notes_files = list(Path(base_path).rglob('*_detailed_notes.md'))
    print(f"\nğŸ“ ç”Ÿæˆçš„ç­†è¨˜æ–‡ä»¶: {len(notes_files)} å€‹")
    
    # æŒ‰æ–‡ä»¶å¤¾åˆ†çµ„é¡¯ç¤º
    folder_notes = {}
    for note_file in notes_files:
        folder_name = note_file.parent.name
        if folder_name not in folder_notes:
            folder_notes[folder_name] = []
        folder_notes[folder_name].append(note_file.name)
    
    print("\nğŸ“ å„æ–‡ä»¶å¤¾çš„ç­†è¨˜ï¼š")
    for i, (folder, notes) in enumerate(sorted(folder_notes.items()), 1):
        print(f"\n{i}. {folder}")
        for note in notes:
            print(f"   - {note}")
    
    # è™•ç†çš„æ–‡ä»¶é¡å‹çµ±è¨ˆ
    txt_count = sum(1 for f in progress['processed'] if f.endswith('.txt'))
    srt_count = sum(1 for f in progress['processed'] if f.endswith('.srt'))
    
    print(f"\nğŸ“Š æ–‡ä»¶é¡å‹çµ±è¨ˆï¼š")
    print(f"  TXT æ–‡ä»¶: {txt_count} å€‹")
    print(f"  SRT æ–‡ä»¶: {srt_count} å€‹")
    
    # æ‰¾å‡ºæœ‰è­°ç¨‹çš„æ–‡ä»¶
    agenda_count = 0
    for note_file in notes_files:
        with open(note_file, 'r', encoding='utf-8') as f:
            content = f.read(500)  # è®€å–å‰500å­—ç¬¦
            if '*åƒè€ƒè­°ç¨‹ï¼š' in content:
                agenda_count += 1
    
    print(f"\nğŸ“‹ è­°ç¨‹åŒ¹é…ï¼š")
    print(f"  æœ‰è­°ç¨‹æ–‡ä»¶: {agenda_count} å€‹")
    print(f"  ç„¡è­°ç¨‹æ–‡ä»¶: {len(notes_files) - agenda_count} å€‹")
    
    # ä¿å­˜æœ€çµ‚å ±å‘Š
    report = {
        'timestamp': datetime.now().isoformat(),
        'statistics': {
            'total_processed': len(progress['processed']),
            'total_failed': len(progress['failed']),
            'total_tokens': progress['stats'].get('total_tokens', 0),
            'estimated_cost_usd': round(estimated_cost, 2),
            'notes_generated': len(notes_files),
            'with_agenda': agenda_count,
            'file_types': {
                'txt': txt_count,
                'srt': srt_count
            }
        },
        'folders_processed': list(folder_notes.keys())
    }
    
    with open('transcription_notes_final_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print("\nâœ… ç¸½çµï¼š")
    print(f"  1. æˆåŠŸè™•ç†äº† {len(progress['processed'])} å€‹è½‰éŒ„æ–‡ä»¶")
    print(f"  2. ç”Ÿæˆäº† {len(notes_files)} å€‹è©³ç´°æ¼”è¬›ç­†è¨˜")
    print(f"  3. ä½¿ç”¨äº† {total_tokens:,} tokens (ç´„ ${estimated_cost:.2f})")
    print(f"  4. æ‰€æœ‰ç­†è¨˜éƒ½å·²ä¿å­˜ç‚º Markdown æ ¼å¼")
    print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜åˆ°: transcription_notes_final_report.json")


if __name__ == "__main__":
    main()