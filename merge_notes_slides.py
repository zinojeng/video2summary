#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åˆä½µæ¼”è¬›ç­†è¨˜èˆ‡æŠ•å½±ç‰‡åˆ†æ
å°‡æ¼”è¬›è€…å…§å®¹èˆ‡æŠ•å½±ç‰‡å…§å®¹æ•´åˆæˆæ›´è©³ç´°çš„äºŒåˆä¸€ç­†è¨˜
"""

import os
import sys
import json
import re
import time
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import google.generativeai as genai
from datetime import datetime


def setup_gemini(api_key: str):
    """è¨­ç½® Gemini API"""
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.5-pro')
    return model


def find_matching_files(base_path: str) -> List[Tuple[Path, Path, Path]]:
    """æŸ¥æ‰¾åŒæ™‚æœ‰æ¼”è¬›ç­†è¨˜å’ŒæŠ•å½±ç‰‡åˆ†æçš„æ–‡ä»¶å¤¾"""
    matches = []
    
    # æŸ¥æ‰¾æ‰€æœ‰æ¼”è¬›ç­†è¨˜
    for notes_file in Path(base_path).rglob('transcription-*_detailed_notes.md'):
        if notes_file.name.startswith('._'):
            continue
            
        folder = notes_file.parent
        
        # æŸ¥æ‰¾å°æ‡‰çš„æŠ•å½±ç‰‡åˆ†ææ–‡ä»¶
        slides_analysis = None
        
        # å„ªå…ˆæŸ¥æ‰¾ selected_slides_analysis.md
        for slides_folder in folder.rglob('*_slides'):
            selected_analysis = slides_folder / 'selected_slides_analysis.md'
            if selected_analysis.exists():
                slides_analysis = selected_analysis
                break
        
        # å¦‚æœæ²’æœ‰ selected_slides_analysis.mdï¼ŒæŸ¥æ‰¾å…¶ä»–åˆ†ææ–‡ä»¶
        if not slides_analysis:
            for slides_folder in folder.rglob('*_slides'):
                for analysis_file in ['slides_analysis.md', 'selected_slides_analysis_gemini.md', 'slides_analysis_gemini.md']:
                    candidate = slides_folder / analysis_file
                    if candidate.exists():
                        slides_analysis = candidate
                        break
                if slides_analysis:
                    break
        
        if slides_analysis:
            matches.append((folder, notes_file, slides_analysis))
    
    return matches


def read_content(file_path: Path) -> str:
    """è®€å–æ–‡ä»¶å…§å®¹"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        print(f"è®€å–æ–‡ä»¶éŒ¯èª¤ {file_path}: {e}")
        return ""


def merge_notes_with_slides(
    model,
    speaker_notes: str,
    slides_analysis: str,
    session_title: str
) -> Tuple[bool, str, Dict]:
    """ä½¿ç”¨ Gemini åˆä½µæ¼”è¬›ç­†è¨˜èˆ‡æŠ•å½±ç‰‡åˆ†æ"""
    
    prompt = f"""è«‹å°‡ä»¥ä¸‹æ¼”è¬›ç­†è¨˜èˆ‡æŠ•å½±ç‰‡åˆ†æåˆä½µæˆä¸€ä»½æ›´è©³ç´°çš„äºŒåˆä¸€ç­†è¨˜ã€‚

è¦æ±‚ï¼š
1. **ä»¥æ¼”è¬›è€…å…§å®¹ç‚ºä¸»è»¸**ï¼Œä¿æŒåŸæœ‰çš„æ¼”è¬›æµç¨‹å’Œçµæ§‹
2. åœ¨ç›¸é—œæ®µè½ä¸­åŠ å…¥æŠ•å½±ç‰‡åƒè€ƒï¼Œæ ¼å¼ï¼š**(åƒè¦‹ Slide X)**
3. ç•¶æŠ•å½±ç‰‡åŒ…å«æ¼”è¬›ä¸­æœªè©³è¿°çš„å…§å®¹æ™‚ï¼Œåœ¨è©²æ®µè½ä¸‹æ–¹ç”¨ __åº•ç·šæ¨™è¨˜__ è£œå……èªªæ˜
4. é¿å…é‡è¤‡å…§å®¹ï¼Œåªè£œå……æ–°çš„è³‡è¨Šæˆ–æ›´æ¸…æ¥šçš„è§£é‡‹
5. ä¿æŒåŸæœ‰çš„éšå±¤çµæ§‹å’Œé‡é»æ¨™è¨˜
6. æŠ•å½±ç‰‡ä¸­çš„åœ–è¡¨ã€æ•¸æ“šæˆ–å…¬å¼ç­‰è¦–è¦ºå…ƒç´ ï¼Œç”¨æ–‡å­—æè¿°è£œå……
7. å»ºç«‹æ¼”è¬›å…§å®¹èˆ‡æŠ•å½±ç‰‡çš„å°æ‡‰é—œä¿‚

æœƒè­°æ¨™é¡Œï¼š{session_title}

---

æ¼”è¬›ç­†è¨˜å…§å®¹ï¼š
{speaker_notes[:40000]}

---

æŠ•å½±ç‰‡åˆ†æå…§å®¹ï¼š
{slides_analysis[:20000]}

---

è«‹ç”Ÿæˆåˆä½µå¾Œçš„è©³ç´°ç­†è¨˜ï¼ˆä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼‰ã€‚è¨˜ä½ï¼š
- ä»¥æ¼”è¬›è€…å…§å®¹ç‚ºä¸»è»¸
- æŠ•å½±ç‰‡å…§å®¹ä½œç‚ºè£œå……å’Œå¢å¼·
- ç”¨åº•ç·šæ¨™è¨˜å»¶ä¼¸è§£è®€
- å»ºç«‹æ¸…æ™°çš„å…§å®¹å°æ‡‰é—œä¿‚
"""
    
    try:
        response = model.generate_content(prompt)
        
        if response.text:
            return True, response.text, {
                'prompt_tokens': len(prompt),
                'completion_tokens': len(response.text),
                'model': 'gemini-2.5-pro'
            }
        else:
            return False, "No response from Gemini", {}
            
    except Exception as e:
        return False, str(e), {}


def save_progress(progress_file: str, progress: Dict):
    """ä¿å­˜é€²åº¦"""
    with open(progress_file, 'w', encoding='utf-8') as f:
        json.dump(progress, f, ensure_ascii=False, indent=2)


def load_progress(progress_file: str) -> Dict:
    """è¼‰å…¥é€²åº¦"""
    if os.path.exists(progress_file):
        with open(progress_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {'processed': [], 'failed': [], 'stats': {}}


def main():
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python merge_notes_slides.py <base_path> <gemini_api_key>")
        sys.exit(1)
    
    base_path = sys.argv[1]
    api_key = sys.argv[2]
    
    print("\nğŸ“ æ‰¹æ¬¡åˆä½µæ¼”è¬›ç­†è¨˜èˆ‡æŠ•å½±ç‰‡åˆ†æ")
    print("="*60)
    print(f"ä½¿ç”¨æ¨¡å‹: Gemini 2.5 Pro")
    print(f"è™•ç†è·¯å¾‘: {base_path}")
    print("="*60)
    
    # è¨­ç½® Gemini
    model = setup_gemini(api_key)
    
    # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
    matches = find_matching_files(base_path)
    print(f"\næ‰¾åˆ° {len(matches)} å€‹åŒæ™‚æœ‰æ¼”è¬›ç­†è¨˜å’ŒæŠ•å½±ç‰‡åˆ†æçš„æ–‡ä»¶å¤¾")
    
    if not matches:
        print("æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…çš„æ–‡ä»¶")
        return
    
    # é¡¯ç¤ºæ‰¾åˆ°çš„åŒ¹é…
    print("\nåŒ¹é…çš„æ–‡ä»¶ï¼š")
    for i, (folder, notes, slides) in enumerate(matches[:5], 1):
        print(f"{i}. {folder.name}")
        print(f"   æ¼”è¬›ç­†è¨˜: {notes.name}")
        print(f"   æŠ•å½±ç‰‡åˆ†æ: {slides.parent.name}/{slides.name}")
    if len(matches) > 5:
        print(f"... é‚„æœ‰ {len(matches) - 5} å€‹æ–‡ä»¶å¤¾")
    
    # è¼‰å…¥é€²åº¦
    progress_file = 'merge_notes_progress.json'
    progress = load_progress(progress_file)
    
    # é–‹å§‹è™•ç†
    start_time = time.time()
    processed_count = 0
    failed_count = 0
    
    for i, (folder, notes_file, slides_file) in enumerate(matches, 1):
        folder_path = str(folder)
        
        # æª¢æŸ¥æ˜¯å¦å·²è™•ç†
        if folder_path in progress['processed']:
            print(f"\n[{i}/{len(matches)}] å·²è™•ç†é: {folder.name}")
            continue
        
        print(f"\n[{i}/{len(matches)}] è™•ç†: {folder.name}")
        
        try:
            # è®€å–å…§å®¹
            print("  è®€å–æ¼”è¬›ç­†è¨˜...")
            speaker_notes = read_content(notes_file)
            
            print("  è®€å–æŠ•å½±ç‰‡åˆ†æ...")
            slides_analysis = read_content(slides_file)
            
            if not speaker_notes or not slides_analysis:
                print("  âš ï¸  æ–‡ä»¶å…§å®¹ç‚ºç©ºï¼Œè·³é")
                continue
            
            print(f"  æ¼”è¬›ç­†è¨˜é•·åº¦: {len(speaker_notes)} å­—ç¬¦")
            print(f"  æŠ•å½±ç‰‡åˆ†æé•·åº¦: {len(slides_analysis)} å­—ç¬¦")
            
            # åˆä½µå…§å®¹
            print("  ç”Ÿæˆåˆä½µç­†è¨˜...")
            success, merged_content, info = merge_notes_with_slides(
                model,
                speaker_notes,
                slides_analysis,
                folder.name
            )
            
            if success:
                # ä¿å­˜åˆä½µç­†è¨˜
                output_file = notes_file.with_name(
                    notes_file.stem.replace('_detailed_notes', '_merged_notes')
                )
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(f"# {folder.name} - æ¼”è¬›èˆ‡æŠ•å½±ç‰‡ç¶œåˆç­†è¨˜\n\n")
                    f.write(f"*æ•´åˆè‡ªæ¼”è¬›ç­†è¨˜èˆ‡æŠ•å½±ç‰‡åˆ†æ*\n\n")
                    f.write(f"*ç”Ÿæˆæ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n\n")
                    f.write(f"*æ¼”è¬›ç­†è¨˜ä¾†æºï¼š{notes_file.name}*\n")
                    f.write(f"*æŠ•å½±ç‰‡åˆ†æä¾†æºï¼š{slides_file.parent.name}/{slides_file.name}*\n\n")
                    f.write("---\n\n")
                    f.write(merged_content)
                
                print(f"  âœ… åˆä½µç­†è¨˜å·²ä¿å­˜: {output_file.name}")
                progress['processed'].append(folder_path)
                processed_count += 1
                
                # æ›´æ–°çµ±è¨ˆ
                if 'total_tokens' not in progress['stats']:
                    progress['stats']['total_tokens'] = 0
                progress['stats']['total_tokens'] += info.get('prompt_tokens', 0) + info.get('completion_tokens', 0)
                
            else:
                print(f"  âŒ è™•ç†å¤±æ•—: {merged_content}")
                progress['failed'].append({
                    'folder': folder_path,
                    'error': merged_content,
                    'timestamp': datetime.now().isoformat()
                })
                failed_count += 1
            
            # ä¿å­˜é€²åº¦
            save_progress(progress_file, progress)
            
            # é¡¯ç¤ºé€²åº¦
            if i < len(matches):
                elapsed = time.time() - start_time
                avg_time = elapsed / i
                eta = avg_time * (len(matches) - i)
                print(f"\né€²åº¦: {i}/{len(matches)} ({i/len(matches)*100:.1f}%)")
                print(f"é è¨ˆå‰©é¤˜æ™‚é–“: {eta/60:.1f} åˆ†é˜")
            
            # å»¶é²
            time.sleep(3)
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸  ç”¨æˆ¶ä¸­æ–·ï¼é€²åº¦å·²ä¿å­˜ã€‚")
            save_progress(progress_file, progress)
            break
        except Exception as e:
            print(f"  âŒ éŒ¯èª¤: {str(e)}")
            progress['failed'].append({
                'folder': folder_path,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            })
            failed_count += 1
            save_progress(progress_file, progress)
    
    # å®Œæˆçµ±è¨ˆ
    total_time = time.time() - start_time
    print("\n" + "="*60)
    print("ğŸ“Š è™•ç†å®Œæˆçµ±è¨ˆ")
    print("="*60)
    print(f"æœ¬æ¬¡è™•ç†: {processed_count} å€‹æˆåŠŸ, {failed_count} å€‹å¤±æ•—")
    print(f"ç¸½å…±è™•ç†: {len(progress['processed'])} å€‹æ–‡ä»¶å¤¾")
    print(f"ç¸½ç”¨æ™‚: {total_time/60:.1f} åˆ†é˜")
    
    if processed_count > 0:
        print(f"å¹³å‡è™•ç†æ™‚é–“: {total_time/processed_count:.1f} ç§’/æ–‡ä»¶å¤¾")
    
    if 'total_tokens' in progress['stats']:
        print(f"ç¸½ Token ä½¿ç”¨: {progress['stats']['total_tokens']:,}")
    
    print(f"\nâœ… æ‰¹æ¬¡è™•ç†å®Œæˆï¼")
    print(f"åˆä½µç­†è¨˜å·²ä¿å­˜ç‚º: *_merged_notes.md")
    print(f"é€²åº¦æ–‡ä»¶: {progress_file}")


if __name__ == "__main__":
    main()